{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0   @user when a father is dysfunctional and is s...\n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  #model   i love u take with u all the time in ...\n",
      "4   5      0             factsguide: society now    #motivation\n",
      "      id                                              tweet\n",
      "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
      "1  31964   @user #white #supremacists want everyone to s...\n",
      "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
      "3  31966  is the hp and the cursed child book up for res...\n",
      "4  31967    3rd #bihday to my amazing, hilarious #nephew...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#create dataframe from csv\n",
    "train_tweets = pd.read_csv(\"/Users/abhilashyelahankaramanjaneya/Desktop/Machine_learning_projects/Projects/Datasets/twitter-train.csv\", sep=',')\n",
    "test_tweets = pd.read_csv(\"/Users/abhilashyelahankaramanjaneya/Desktop/Machine_learning_projects/Projects/Datasets/twitter-test.csv\", sep=',')\n",
    "print(train_tweets.head())\n",
    "print(test_tweets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tok(data):\n",
    "#     words = data[\"tweet\"]\n",
    "#     tokens = nltk.word_tokenize(words)\n",
    "#     token_words = [w for w in tokens if w.isalpha()]\n",
    "#     return token_words\n",
    "\n",
    "# data1 = train_tweets.apply(tok, axis = 1)\n",
    "# print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# stemming = PorterStemmer()\n",
    "# def stem(data):\n",
    "#     stemmed = [stemming.stem(word) for word in data]\n",
    "#     return stemmed\n",
    "\n",
    "# data1 = data1.apply(stem)\n",
    "# print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# def stop_word(data):\n",
    "#     stop_words = [w for w in data if not w in stops]\n",
    "#     return stop_words\n",
    "\n",
    "# data1 = data1.apply(stop_word)\n",
    "# print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import LancasterStemmer\n",
    "# lemmatize = LancasterStemmer()\n",
    "\n",
    "# def lemmat(data):\n",
    "#     lemmatized = [lemmatize.stem(w) for w in data]\n",
    "#     return lemmatized\n",
    "\n",
    "# data1 = data1.apply(lemmat)\n",
    "# print(data1.iloc[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pos_tag(data):\n",
    "#     pos_tagged = [nltk.pos_tag(w) for w in data]\n",
    "#     return pos_tagged\n",
    "\n",
    "# data1 = data1.apply(pos_tag)\n",
    "# print(data1.iloc[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The train tweets are \n",
      " :    id  label                                              tweet\n",
      "0   1      0  [user, when, a, father, is, dysfunctional, and...\n",
      "1   2      0  [user, user, thanks, for, lyft, credit, i, ca,...\n",
      "2   3      0                            [bihday, your, majesty]\n",
      "3   4      0  [model, i, love, u, take, with, u, all, the, t...\n",
      "4   5      0             [factsguide, society, now, motivation] \n",
      " The test tweets are \n",
      " :       id                                              tweet\n",
      "0  31963  [studiolife, aislife, requires, passion, dedic...\n",
      "1  31964  [user, white, supremacists, want, everyone, to...\n",
      "2  31965  [safe, ways, to, heal, your, acne, altwaystohe...\n",
      "3  31966  [is, the, hp, and, the, cursed, child, book, u...\n",
      "4  31967  [bihday, to, my, amazing, hilarious, nephew, e... \n"
     ]
    }
   ],
   "source": [
    "def tokenization(col):\n",
    "    review = col['tweet']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "train_tweets['tweet'] = train_tweets.apply(tokenization, axis = 1)\n",
    "test_tweets['tweet'] = test_tweets.apply(tokenization, axis =1)\n",
    "\n",
    "print(f\" The train tweets are \\n : {train_tweets.head()} \")\n",
    "print(f\" The test tweets are \\n : {test_tweets.head()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The train tweets are \n",
      " :    id  label                                              tweet\n",
      "0   1      0  user when a father is dysfunctional and is so ...\n",
      "1   2      0  user user thanks for lyft credit i ca use caus...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0         model i love u take with u all the time in\n",
      "4   5      0                  factsguide society now motivation \n",
      " The test tweets are \n",
      " :       id                                              tweet\n",
      "0  31963  studiolife aislife requires passion dedication...\n",
      "1  31964  user white supremacists want everyone to see t...\n",
      "2  31965  safe ways to heal your acne altwaystoheal heal...\n",
      "3  31966  is the hp and the cursed child book up for res...\n",
      "4  31967  bihday to my amazing hilarious nephew eli ahmi... \n"
     ]
    }
   ],
   "source": [
    "def rejoin_words(col):\n",
    "    my_list = col['tweet']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "train_tweets['tweet'] = train_tweets.apply(rejoin_words, axis = 1)\n",
    "test_tweets['tweet'] = test_tweets.apply(rejoin_words, axis =1)\n",
    "\n",
    "print(f\" The train tweets are \\n : {train_tweets.head()} \")\n",
    "print(f\" The test tweets are \\n : {test_tweets.head()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tweets['tweet'] = train_tweets['tweet'].astype(str)\n",
    "# train_tweets = train_tweets.dropna()\n",
    "# test_tweets['tweet'] = test_tweets['tweet'].astype(str)\n",
    "# train_tweets['tweet'] = train_tweets['tweet'].replace('!@#$%^&*\\(|\\):;.,<>?/', '', regex = True)\n",
    "# test_tweets['tweet'] = test_tweets['tweet'].replace('!@#$%^&*\\(|\\):;.,<>?/', '', regex = True)\n",
    "# train_tweets = train_tweets.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "# test_tweets = test_tweets.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "# test_tweets= test_tweets.dropna()\n",
    "# print(f\" The train tweets are \\n : {train_tweets.head()} \")\n",
    "# print(f\" The test tweets are \\n : {test_tweets.head()} \")\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet label\n",
      "0  31963  studiolife aislife requires passion dedication...      \n",
      "1  31964  user white supremacists want everyone to see t...      \n",
      "2  31965  safe ways to heal your acne altwaystoheal heal...      \n",
      "3  31966  is the hp and the cursed child book up for res...      \n",
      "4  31967  bihday to my amazing hilarious nephew eli ahmi...      \n"
     ]
    }
   ],
   "source": [
    "test_tweets[\"label\"] = \"\"\n",
    "print(test_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-88ca7f7b86cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# clean the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Machine_learning_projects/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-88ca7f7b86cc>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# clean the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from gensim.parsing import remove_stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "\n",
    "from textblob import classifiers\n",
    "\n",
    "\n",
    "train_tweets = train_tweets[:1000]\n",
    "\n",
    "tweets = train_tweets['tweet']\n",
    "labels = train_tweets['label']\n",
    "\n",
    "# clean the data set\n",
    "tweets = tweets.apply(lambda x : clean_data(x))\n",
    "\n",
    "tweets = tweets.tolist()\n",
    "labels = labels.tolist()\n",
    "\n",
    "\n",
    "# *****************************************************************************\n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, labels, test_size=0.3, random_state=100,\n",
    "                                                                      stratify=labels)\n",
    "\n",
    "\n",
    "# We need to convert the train and test data into a list of tuples of the type (tweet,label)\n",
    "training_corpus = list(zip(tweets_train,labels_train))\n",
    "test_corpus = list(zip(tweets_test,labels_test))\n",
    "\n",
    "print(\"Training classifier......\")\n",
    "classifier = classifiers.DecisionTreeClassifier(training_corpus)\n",
    "\n",
    "predictions = []\n",
    "for tweet in tweets_test:\n",
    "    pred = classifier.classify(tweet)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "print(\"F1 score=\" , f1_score(labels_test,predictions))\n",
    "print(\"ROC AUC score = \", roc_auc_score(labels_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
