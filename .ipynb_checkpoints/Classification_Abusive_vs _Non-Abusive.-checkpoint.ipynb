{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0   @user when a father is dysfunctional and is s...\n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  #model   i love u take with u all the time in ...\n",
      "4   5      0             factsguide: society now    #motivation\n",
      "      id                                              tweet\n",
      "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
      "1  31964   @user #white #supremacists want everyone to s...\n",
      "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
      "3  31966  is the hp and the cursed child book up for res...\n",
      "4  31967    3rd #bihday to my amazing, hilarious #nephew...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#create dataframe from csv\n",
    "train_tweets = pd.read_csv(\"/Users/abhilashyelahankaramanjaneya/Desktop/Machine_learning_projects/Projects/Datasets/twitter-train.csv\", sep=',')\n",
    "test_tweets = pd.read_csv(\"/Users/abhilashyelahankaramanjaneya/Desktop/Machine_learning_projects/Projects/Datasets/twitter-test.csv\", sep=',')\n",
    "print(train_tweets.head())\n",
    "print(test_tweets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0  [user, when, a, father, is, dysfunctional, and...\n",
      "1   2      0  [user, user, thanks, for, lyft, credit, i, ca,...\n",
      "2   3      0                            [bihday, your, majesty]\n",
      "3   4      0  [model, i, love, u, take, with, u, all, the, t...\n",
      "4   5      0             [factsguide, society, now, motivation]\n"
     ]
    }
   ],
   "source": [
    "def tok(data):\n",
    "    words = data[\"tweet\"]\n",
    "    tokens = nltk.word_tokenize(words)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "train_tweets['tweet'] = train_tweets.apply(tok, axis = 1)\n",
    "print(train_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0  [user, when, a, father, is, dysfunct, and, is,...\n",
      "1   2      0  [user, user, thank, for, lyft, credit, i, ca, ...\n",
      "2   3      0                            [bihday, your, majesti]\n",
      "3   4      0  [model, i, love, u, take, with, u, all, the, t...\n",
      "4   5      0                   [factsguid, societi, now, motiv]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "def stem(data):\n",
    "    stemmed = [stemming.stem(word) for word in data]\n",
    "    return stemmed\n",
    "\n",
    "train_tweets['tweet'] = train_tweets['tweet'].apply(stem)\n",
    "print(train_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0  [user, father, dysfunct, selfish, drag, hi, ki...\n",
      "1   2      0  [user, user, thank, lyft, credit, ca, use, cau...\n",
      "2   3      0                                  [bihday, majesti]\n",
      "3   4      0                    [model, love, u, take, u, time]\n",
      "4   5      0                        [factsguid, societi, motiv]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def stop_word(data):\n",
    "    stop_words = [w for w in data if not w in stops]\n",
    "    return stop_words\n",
    "\n",
    "train_tweets['tweet'] = train_tweets['tweet'].apply(stop_word)\n",
    "print(train_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label                                              tweet\n",
      "0   1      0  [us, fath, dysfunct, self, drag, hi, kid, hi, ...\n",
      "1   2      0  [us, us, thank, lyft, credit, ca, us, cau, off...\n",
      "2   3      0                                   [bihday, majest]\n",
      "3   4      0                       [model, lov, u, tak, u, tim]\n",
      "4   5      0                           [factsguid, societ, mot]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lemmatize = LancasterStemmer()\n",
    "\n",
    "def lemmat(data):\n",
    "    lemmatized = [lemmatize.stem(w) for w in data]\n",
    "    return lemmatized\n",
    "\n",
    "train_tweets['tweet'] = train_tweets['tweet'].apply(lemmat)\n",
    "print(train_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pos_tag(data):\n",
    "#     pos_tagged = [nltk.pos_tag(w) for w in data]\n",
    "#     return pos_tagged\n",
    "\n",
    "# data1 = data1.apply(pos_tag)\n",
    "# print(data1.iloc[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenization(col):\n",
    "#     review = col['tweet']\n",
    "#     tokens = nltk.word_tokenize(review)\n",
    "#     # taken only words (not punctuation)\n",
    "#     token_words = [w for w in tokens if w.isalpha()]\n",
    "#     return token_words\n",
    "# train_tweets['tweet'] = train_tweets.apply(tokenization, axis = 1)\n",
    "# test_tweets['tweet'] = test_tweets.apply(tokenization, axis =1)\n",
    "\n",
    "# print(f\" The train tweets are \\n : {train_tweets.head()} \")\n",
    "# print(f\" The test tweets are \\n : {test_tweets.head()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The train tweets are \n",
      " :    id  label                                              tweet\n",
      "0   1      0  us fath dysfunct self drag hi kid hi dysfunct run\n",
      "1   2      0  us us thank lyft credit ca us cau off wheelcha...\n",
      "2   3      0                                      bihday majest\n",
      "3   4      0                              model lov u tak u tim\n",
      "4   5      0                               factsguid societ mot \n"
     ]
    }
   ],
   "source": [
    "def rejoin_words(col):\n",
    "    my_list = col\n",
    "    joined_words = (\" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "train_tweets['tweet'] = train_tweets['tweet'].apply(rejoin_words)\n",
    "# test_tweets['tweet'] = test_tweets.apply(rejoin_words, axis =1)\n",
    "\n",
    "print(f\" The train tweets are \\n : {train_tweets.head()} \")\n",
    "# print(f\" The test tweets are \\n : {test_tweets.head()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tweets['tweet'] = train_tweets['tweet'].astype(str)\n",
    "# train_tweets = train_tweets.dropna()\n",
    "# test_tweets['tweet'] = test_tweets['tweet'].astype(str)\n",
    "# train_tweets['tweet'] = train_tweets['tweet'].replace('!@#$%^&*\\(|\\):;.,<>?/', '', regex = True)\n",
    "# test_tweets['tweet'] = test_tweets['tweet'].replace('!@#$%^&*\\(|\\):;.,<>?/', '', regex = True)\n",
    "# train_tweets = train_tweets.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "# test_tweets = test_tweets.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "# test_tweets= test_tweets.dropna()\n",
    "# print(f\" The train tweets are \\n : {train_tweets.head()} \")\n",
    "# print(f\" The test tweets are \\n : {test_tweets.head()} \")\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tweets[\"label\"] = \"\"\n",
    "# print(test_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier......\n",
      "F1 score= 0.3783783783783784\n",
      "ROC AUC score =  0.644702419882276\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from gensim.parsing import remove_stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "\n",
    "from textblob import classifiers\n",
    "\n",
    "\n",
    "train_tweets = train_tweets\n",
    "\n",
    "tweets = train_tweets['tweet']\n",
    "labels = train_tweets['label']\n",
    "\n",
    "# clean the data set\n",
    "# tweets = tweets.apply(lambda x : clean_data(x))\n",
    "\n",
    "# tweets = tweets.tolist()\n",
    "# labels = labels.tolist()\n",
    "\n",
    "\n",
    "# *****************************************************************************\n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, labels, test_size=0.3, random_state=100,\n",
    "                                                                      stratify=labels)\n",
    "\n",
    "\n",
    "# We need to convert the train and test data into a list of tuples of the type (tweet,label)\n",
    "training_corpus = list(zip(tweets_train,labels_train))\n",
    "test_corpus = list(zip(tweets_test,labels_test))\n",
    "\n",
    "print(\"Training classifier......\")\n",
    "classifier = classifiers.DecisionTreeClassifier(training_corpus)\n",
    "\n",
    "predictions = []\n",
    "for tweet in tweets_test:\n",
    "    pred = classifier.classify(tweet)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "print(\"F1 score=\" , f1_score(labels_test,predictions))\n",
    "print(\"ROC AUC score = \", roc_auc_score(labels_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 2241)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(tweets_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 2241)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 2241)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(tweets_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(tweets_test)\n",
    "np.mean(predicted == labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9233333333333333"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "    alpha=1e-3, random_state=42,\n",
    "    max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(tweets_train, labels_train)\n",
    "predicted = text_clf.predict(tweets_test)\n",
    "np.mean(predicted == labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "Precision: 0.43\n",
      "Recall: 0.14\n",
      "F1_score: 0.21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "print(f\"Accuracy: {round(accuracy_score(labels_test, predicted), 2)}\") \n",
    "print(f\"Precision: {round(precision_score(labels_test, predicted), 2)}\")\n",
    "print(f\"Recall: {round(recall_score(labels_test, predicted), 2)}\")\n",
    "print(f\"F1_score: {round(f1_score(labels_test, predicted), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       278\n",
      "           1       0.43      0.14      0.21        22\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.68      0.56      0.58       300\n",
      "weighted avg       0.90      0.92      0.90       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# gnb = GaussianNB()\n",
    "# gnb.fit(tweets_train, labels_train)\n",
    "\n",
    "# # predict\n",
    "# y_pred = gnb.predict(tweets_test)\n",
    "\n",
    "# # f1 score\n",
    "# score = f1_score(y_pred, labels_test)\n",
    "\n",
    "# # # print\n",
    "# print(\"Naives Bayes F1 score is \",score)\n",
    "\n",
    "    \n",
    "# # print(\"F1 score=\" , f1_score(labels_test,predictions))\n",
    "# # print(\"ROC AUC score = \", roc_auc_score(labels_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
